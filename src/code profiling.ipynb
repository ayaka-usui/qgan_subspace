{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "895f5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.training_init import run_single_training\n",
    "from config import CFG\n",
    "\n",
    "config = CFG\n",
    "config.epochs = 1\n",
    "config.iterations_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "696023bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f340b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qgan.training\n",
    "import qgan.cost_functions import compute_fidelity_and_cost, compute_cost\n",
    "import qgan.discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbe8a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in SINGLE RUN mode.\n",
      "\n",
      "\n",
      "================================================== \n",
      "run_timestamp: 2025-10-07__15-30-05,\n",
      "----------------------------------------------\n",
      "load_timestamp: None,\n",
      "type_of_warm_start: none,\n",
      "warm_start_strength: 0.1,\n",
      "----------------------------------------------\n",
      "system_size: 3,\n",
      "extra_ancilla: False,\n",
      "ancilla_mode: pass,\n",
      "ancilla_project_norm: re-norm,\n",
      "ancilla_topology: bridge,\n",
      "ancilla_connect_to: None,\n",
      "do_ancilla_1q_gates: True,\n",
      "start_ancilla_gates_randomly: True,\n",
      "----------------------------------------------\n",
      "gen_layers: 3,\n",
      "gen_ansatz: ZZ_X_Z,\n",
      "----------------------------------------------\n",
      "target_hamiltonian: custom_h,\n",
      "custom_hamiltonian_terms: ['ZZZ'],\n",
      "custom_hamiltonian_strengths: [1.0],\n",
      "----------------------------------------------\n",
      "epochs: 1,\n",
      "iterations_epoch: 300,\n",
      "log_every_x_iter: 10,\n",
      "save_fid_and_loss_every_x_iter: 1,\n",
      "max_fidelity: 0.99,\n",
      "steps_gen: 1,\n",
      "steps_dis: 1,\n",
      "----------------------------------------------\n",
      "l_rate: 0.01,\n",
      "momentum_coeff: 0.9,\n",
      "================================================== \n",
      "\n",
      "\n",
      "Epoch:    1 | Iter:    1 | Fidelity: 0.042023 | Loss: -4.142934\n",
      "\n",
      "Epoch:    1 | Iter:   11 | Fidelity: 0.043433 | Loss: -3.519916\n",
      "\n",
      "Epoch:    1 | Iter:   21 | Fidelity: 0.045875 | Loss: -3.483936\n",
      "\n",
      "Epoch:    1 | Iter:   31 | Fidelity: 0.048387 | Loss: -3.467923\n",
      "\n",
      "Epoch:    1 | Iter:   41 | Fidelity: 0.050431 | Loss: -3.423178\n",
      "\n",
      "Epoch:    1 | Iter:   51 | Fidelity: 0.052172 | Loss: -3.297071\n",
      "\n",
      "Epoch:    1 | Iter:   61 | Fidelity: 0.052786 | Loss: -3.027411\n",
      "\n",
      "Epoch:    1 | Iter:   71 | Fidelity: 0.025699 | Loss: -3.267996\n",
      "\n",
      "Epoch:    1 | Iter:   81 | Fidelity: 0.014512 | Loss: -3.163136\n",
      "\n",
      "Epoch:    1 | Iter:   91 | Fidelity: 0.067750 | Loss: -2.571821\n",
      "\n",
      "Epoch:    1 | Iter:  101 | Fidelity: 0.100597 | Loss: -1.613223\n",
      "\n",
      "Epoch:    1 | Iter:  111 | Fidelity: 0.122717 | Loss: -0.167251\n",
      "\n",
      "Epoch:    1 | Iter:  121 | Fidelity: 0.142715 | Loss: 0.329658\n",
      "\n",
      "Epoch:    1 | Iter:  131 | Fidelity: 0.164564 | Loss: 0.338868\n",
      "\n",
      "Epoch:    1 | Iter:  141 | Fidelity: 0.188674 | Loss: 0.355989\n",
      "\n",
      "Epoch:    1 | Iter:  151 | Fidelity: 0.214465 | Loss: 0.372205\n",
      "\n",
      "Epoch:    1 | Iter:  161 | Fidelity: 0.240889 | Loss: 0.366889\n",
      "\n",
      "Epoch:    1 | Iter:  171 | Fidelity: 0.266604 | Loss: 0.354370\n",
      "\n",
      "Epoch:    1 | Iter:  181 | Fidelity: 0.290668 | Loss: 0.343139\n",
      "\n",
      "Epoch:    1 | Iter:  191 | Fidelity: 0.312570 | Loss: 0.332784\n",
      "\n",
      "Epoch:    1 | Iter:  201 | Fidelity: 0.332262 | Loss: 0.323219\n",
      "\n",
      "Epoch:    1 | Iter:  211 | Fidelity: 0.349956 | Loss: 0.314525\n",
      "\n",
      "Epoch:    1 | Iter:  221 | Fidelity: 0.365928 | Loss: 0.306645\n",
      "\n",
      "Epoch:    1 | Iter:  231 | Fidelity: 0.380403 | Loss: 0.299469\n",
      "\n",
      "Epoch:    1 | Iter:  241 | Fidelity: 0.393510 | Loss: 0.292964\n",
      "\n",
      "Epoch:    1 | Iter:  251 | Fidelity: 0.405308 | Loss: 0.287096\n",
      "\n",
      "Epoch:    1 | Iter:  261 | Fidelity: 0.415824 | Loss: 0.281869\n",
      "\n",
      "Epoch:    1 | Iter:  271 | Fidelity: 0.425086 | Loss: 0.277259\n",
      "\n",
      "Epoch:    1 | Iter:  281 | Fidelity: 0.433144 | Loss: 0.273250\n",
      "\n",
      "Epoch:    1 | Iter:  291 | Fidelity: 0.440072 | Loss: 0.269805\n",
      "\n",
      "Training finished after 1 epochs.\n",
      "\n",
      "Run took: 23.591926399967633 time.\n",
      "Generator model saved to ./generated_data/2025-10-07__15-30-05/saved_model/model-gen(hs).pkl\n",
      "Discriminator model saved to ./generated_data/2025-10-07__15-30-05/saved_model/model-dis(hs).pkl\n",
      "\n",
      "Default configuration run COMPLETED SUCCESSFULLY.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 4.98089 s\n",
      "File: c:\\Users\\f52ga\\CVC\\Codes\\qgan_subspace\\src\\qgan\\cost_functions.py\n",
      "Function: compute_cost at line 21\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    21                                           def compute_cost(dis, final_target_state: torch.Tensor, final_gen_state: torch.Tensor, config=CFG) -> float:\n",
      "    22                                               \"\"\"Calculate the cost function. Which is basically equivalent to the Wasserstein distance.\n",
      "    23                                           \n",
      "    24                                               Args:\n",
      "    25                                                   dis (Discriminator): the discriminator.\n",
      "    26                                                   final_target_state (np.ndarray): the target state to input into the Discriminator.\n",
      "    27                                                   final_gen_state (np.ndarray): the gen state to input into the Discriminator.\n",
      "    28                                                   config (Config): training configuration (defaults to CFG).\n",
      "    29                                               Returns:\n",
      "    30                                                   float: the cost function.\n",
      "    31                                               \"\"\"\n",
      "    32       900   42673201.0  47414.7     85.7      A, B, psi, phi = dis.get_dis_matrices_rep()\n",
      "    33                                               \n",
      "    34                                           \n",
      "    35                                               # fmt: off\n",
      "    36       900      55645.0     61.8      0.1      final_gen_state = final_gen_state.flatten()\n",
      "    37       900      74392.0     82.7      0.1      final_target_state = final_target_state.flatten()\n",
      "    38       900     266307.0    295.9      0.5      A_final_gen_state = A @ final_gen_state\n",
      "    39       900     121321.0    134.8      0.2      B_final_gen_state = B @ final_gen_state\n",
      "    40                                           \n",
      "    41       900     124280.0    138.1      0.2      term1 = torch.vdot(final_gen_state, A_final_gen_state)\n",
      "    42       900     186473.0    207.2      0.4      term2 = torch.vdot(final_target_state, B @ final_target_state)\n",
      "    43                                           \n",
      "    44       900      48725.0     54.1      0.1      term3 = torch.vdot(B_final_gen_state, final_target_state)\n",
      "    45       900      48818.0     54.2      0.1      term4 = torch.vdot(final_target_state, A_final_gen_state)\n",
      "    46                                           \n",
      "    47       900      42311.0     47.0      0.1      term5 = torch.vdot(A_final_gen_state, final_target_state)\n",
      "    48       900      42639.0     47.4      0.1      term6 = torch.vdot(final_target_state, B_final_gen_state)\n",
      "    49                                           \n",
      "    50       900      42481.0     47.2      0.1      term7 = torch.vdot(B_final_gen_state, final_gen_state)\n",
      "    51       900     153303.0    170.3      0.3      term8 = torch.vdot(final_target_state, A @ final_target_state)\n",
      "    52                                           \n",
      "    53       900    2268445.0   2520.5      4.6      psiterm = torch.trace(torch.outer(final_target_state, final_target_state.conj()) @ psi)\n",
      "    54       900    2002370.0   2224.9      4.0      phiterm = torch.trace(torch.outer(final_gen_state, final_gen_state.conj()) @ phi)\n",
      "    55                                           \n",
      "    56       900    1348007.0   1497.8      2.7      regterm = (config.lamb / torch.e) * (config.cst1 * term1 * term2 - config.cst2 * (term3 * term4 + term5 * term6) + config.cst3 * term7 * term8)\n",
      "    57                                               # fmt: on\n",
      "    58                                           \n",
      "    59                                               # The final loss must be a real-valued scalar tensor\n",
      "    60       900     297730.0    330.8      0.6      loss = (psiterm - phiterm - regterm).real\n",
      "    61       900      12443.0     13.8      0.0      return loss\n",
      "\n",
      "Total time: 4.25321 s\n",
      "File: c:\\Users\\f52ga\\CVC\\Codes\\qgan_subspace\\src\\qgan\\discriminator.py\n",
      "Function: get_dis_matrices_rep at line 67\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    67                                               def get_dis_matrices_rep(self) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "    68                                                   \"\"\"\n",
      "    69                                                   Computes the matrices A and B from psi and phi. These are used in the cost function.\n",
      "    70                                                   \"\"\"\n",
      "    71       900   14290464.0  15878.3     33.6          psi, phi = self.forward()\n",
      "    72                                                   \n",
      "    73                                                   # lamb is a hyperparameter from the config\n",
      "    74       900       9186.0     10.2      0.0          lamb = self.config.lamb\n",
      "    75                                                   \n",
      "    76                                                   # A = expm(-1/lamb * phi)\n",
      "    77       900   14722168.0  16358.0     34.6          A = torch.matrix_exp((-1.0 / lamb) * phi)\n",
      "    78                                                   # B = expm(1/lamb * psi)\n",
      "    79       900   13485565.0  14984.0     31.7          B = torch.matrix_exp((1.0 / lamb) * psi)\n",
      "    80                                           \n",
      "    81       900      24686.0     27.4      0.1          return A, B, psi, phi\n",
      "\n",
      "Total time: 10.2812 s\n",
      "File: c:\\Users\\f52ga\\CVC\\Codes\\qgan_subspace\\src\\qgan\\generator.py\n",
      "Function: forward at line 46\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    46                                               def forward(self, total_input_state: torch.Tensor) -> torch.Tensor:\n",
      "    47                                                   \"\"\"\n",
      "    48                                                   The forward pass of the generator. It applies the quantum circuit\n",
      "    49                                                   to the input state.\n",
      "    50                                                   \"\"\"\n",
      "    51       600  102811983.0 171353.3    100.0          return self.ansatz(total_input_state.flatten()).to(total_input_state.device)\n",
      "\n",
      "Total time: 23.5916 s\n",
      "File: c:\\Users\\f52ga\\CVC\\Codes\\qgan_subspace\\src\\qgan\\training.py\n",
      "Function: run at line 65\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    65                                               def run(self):\n",
      "    66                                                   \"\"\"Runs the entire QGAN training loop.\"\"\"\n",
      "    67         1       6990.0   6990.0      0.0          print_and_log(\"\\n\" + self.config.show_data(), self.config.log_path)\n",
      "    68                                                   \n",
      "    69                                                   # Note: Model loading logic would need to be adapted to use `load_state_dict`\n",
      "    70                                                   # For simplicity in this migration, we are starting from scratch.\n",
      "    71                                                   # load_models_if_specified(self) \n",
      "    72                                           \n",
      "    73         1          9.0      9.0      0.0          fidelities_history, losses_history, runtimes_history = [], [], []\n",
      "    74         1         18.0     18.0      0.0          starttime = tpc()\n",
      "    75                                                   \n",
      "    76         2         29.0     14.5      0.0          for epoch in range(1, self.config.epochs + 1):\n",
      "    77         1          5.0      5.0      0.0              epoch_fidelities, epoch_losses, epoch_runtimes = [], [], []\n",
      "    78       301       2573.0      8.5      0.0              for i in range(self.config.iterations_epoch):\n",
      "    79                                                           # --- Train Discriminator ---\n",
      "    80                                                           # We need to detach the generator's output from the computation graph\n",
      "    81                                                           # when training the discriminator to avoid backpropagating through the generator.\n",
      "    82                                                           \n",
      "    83       300       6519.0     21.7      0.0                  t0 = tpc()\n",
      "    84                                                           # --- Train Generator ---\n",
      "    85       600      12615.0     21.0      0.0                  for _ in range(self.config.steps_gen):\n",
      "    86       300     628683.0   2095.6      0.3                      self.optimizer_gen.zero_grad()\n",
      "    87                                                               # Now, we use the generator's output directly to build the graph\n",
      "    88       300   53659716.0 178865.7     22.7                      total_gen_state = self.gen(self.initial_state_total)\n",
      "    89       300      14656.0     48.9      0.0                      final_gen_state = get_final_gen_state_for_discriminator(total_gen_state)\n",
      "    90                                                               # The generator aims to minimize the cost function\n",
      "    91       300   18630792.0  62102.6      7.9                      gen_loss = compute_cost(self.dis, self.final_target_state, final_gen_state)\n",
      "    92       300   46826929.0 156089.8     19.8                      gen_loss.backward()\n",
      "    93       300    1109634.0   3698.8      0.5                      self.optimizer_gen.step()\n",
      "    94                                           \n",
      "    95       600      89031.0    148.4      0.0                  with torch.no_grad():\n",
      "    96       300   49289737.0 164299.1     20.9                      total_gen_state = self.gen(self.initial_state_total)\n",
      "    97       300      15088.0     50.3      0.0                  final_gen_state_detached = get_final_gen_state_for_discriminator(total_gen_state)\n",
      "    98                                           \n",
      "    99       600       6442.0     10.7      0.0                  for _ in range(self.config.steps_dis):\n",
      "   100       300     559334.0   1864.4      0.2                      self.optimizer_dis.zero_grad()\n",
      "   101                                                               # The cost function for the discriminator aims to maximize the distance,\n",
      "   102                                                               # so we multiply by -1 to perform gradient descent (minimization).\n",
      "   103       300   17887634.0  59625.4      7.6                      dis_loss = -1 * compute_cost(self.dis, self.final_target_state, final_gen_state_detached)\n",
      "   104       300   29990464.0  99968.2     12.7                      dis_loss.backward()\n",
      "   105       300    1179357.0   3931.2      0.5                      self.optimizer_dis.step()\n",
      "   106       300       6620.0     22.1      0.0                  tf = tpc()\n",
      "   107                                                           # --- Logging and Monitoring ---\n",
      "   108       300       5846.0     19.5      0.0                  if i % self.config.save_fid_and_loss_every_x_iter == 0:\n",
      "   109                                                               # Use the detached state for fidelity/loss calculation to save memory\n",
      "   110       300   15554095.0  51847.0      6.6                      fid, loss = compute_fidelity_and_cost(self.dis, self.final_target_state, final_gen_state_detached)\n",
      "   111       300       3405.0     11.3      0.0                      epoch_fidelities.append(fid)\n",
      "   112       300      17964.0     59.9      0.0                      epoch_losses.append(loss.item()) # .item() gets the float value\n",
      "   113       300       2702.0      9.0      0.0                      epoch_runtimes.append(tf-t0)\n",
      "   114       300       5493.0     18.3      0.0                      if i % self.config.log_every_x_iter == 0:\n",
      "   115        30       4464.0    148.8      0.0                          info = f\"\\nEpoch: {epoch:4d} | Iter: {i+1:4d} | Fidelity: {fid:8f} | Loss: {loss.item():8f}\"\n",
      "   116        30     299722.0   9990.7      0.1                          print_and_log(info, self.config.log_path)\n",
      "   117                                                       \n",
      "   118                                                       # --- Store and Plot Epoch History ---\n",
      "   119         1         42.0     42.0      0.0              fidelities_history.extend(epoch_fidelities)\n",
      "   120         1         15.0     15.0      0.0              losses_history.extend(epoch_losses)\n",
      "   121         1         15.0     15.0      0.0              runtimes_history.extend(epoch_runtimes)\n",
      "   122                                                       #plt_fidelity_vs_iter(np.array(fidelities_history), np.array(losses_history), CFG, epoch)\n",
      "   123                                           \n",
      "   124                                                       # --- Stopping Conditions ---\n",
      "   125         1         17.0     17.0      0.0              if epoch_fidelities and epoch_fidelities[-1] > self.config.max_fidelity:\n",
      "   126                                                           print_and_log(f\"\\nFidelity threshold of {self.config.max_fidelity} reached. Stopping training.\", self.config.log_path)\n",
      "   127                                                           break\n",
      "   128                                                   \n",
      "   129                                                   # --- End of Training ---\n",
      "   130         1      15100.0  15100.0      0.0          print_and_log(f\"\\nTraining finished after {epoch} epochs.\", self.config.log_path)\n",
      "   131         1         20.0     20.0      0.0          endtime = tpc()\n",
      "   132         1       7781.0   7781.0      0.0          print_and_log(f\"\\nRun took: {endtime - starttime} time.\", self.config.log_path)\n",
      "   133                                                   \n",
      "   134                                                   # --- Save Final Results ---\n",
      "   135         1      51214.0  51214.0      0.0          save_fidelity_loss(np.array(fidelities_history), np.array(losses_history), self.config.fid_loss_path)\n",
      "   136         1      13316.0  13316.0      0.0          self.gen.save_model_params(self.config.model_gen_path)\n",
      "   137         1      10983.0  10983.0      0.0          self.dis.save_model_params(self.config.model_dis_path)\n",
      "   138                                           \n",
      "   139         1        628.0    628.0      0.0          return Results(fidelities=np.array(fidelities_history), losses=np.array(losses_history), runtimes=np.array(runtimes_history), total_time=endtime-starttime)\n",
      "\n",
      "Total time: 23.6052 s\n",
      "File: c:\\Users\\f52ga\\CVC\\Codes\\qgan_subspace\\src\\tools\\training_init.py\n",
      "Function: run_single_training at line 35\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    35                                           def run_single_training(config=CFG):\n",
      "    36                                               \"\"\"\n",
      "    37                                               Runs a single training instance.\n",
      "    38                                               \"\"\"\n",
      "    39         1       9514.0   9514.0      0.0      print_and_log(\"Running in SINGLE RUN mode.\\n\", config.log_path)\n",
      "    40                                           \n",
      "    41         1          5.0      5.0      0.0      try:\n",
      "    42                                                   ##############################################################\n",
      "    43                                                   # Run single training instance with specified configuration\n",
      "    44                                                   ##############################################################\n",
      "    45         1      21906.0  21906.0      0.0          training_instance = Training(config=config)\n",
      "    46         1  236015672.0    2e+08    100.0          results = training_instance.run()\n",
      "    47         1          8.0      8.0      0.0          success_msg = \"\\nDefault configuration run COMPLETED SUCCESSFULLY.\\n\"\n",
      "    48         1       4681.0   4681.0      0.0          print_and_log(success_msg, config.log_path)  # Log to file\n",
      "    49         1          6.0      6.0      0.0          return results\n",
      "    50                                           \n",
      "    51                                               except Exception as e:  # noqa: BLE001\n",
      "    52                                                   ##############################################################\n",
      "    53                                                   # Handle exceptions during the training run\n",
      "    54                                                   ##############################################################\n",
      "    55                                                   tb_str = traceback.format_exc()\n",
      "    56                                                   error_msg = (\n",
      "    57                                                       f\"\\n{'-' * 60}\\n\"\n",
      "    58                                                       f\"FAILED: Default configuration run!\\n\"\n",
      "    59                                                       f\"Error Type: {type(e).__name__}\\n\"\n",
      "    60                                                       f\"Error Message: {e!s}\\n\"\n",
      "    61                                                       f\"Traceback:\\n{tb_str}\"\n",
      "    62                                                       f\"{'=' * 60}\\n\"\n",
      "    63                                                   )\n",
      "    64                                                   print_and_log(error_msg, config.log_path)  # Log to file"
     ]
    }
   ],
   "source": [
    "%lprun -f qgan.training.Training.run -f qgan.cost_functions.compute_cost -f qgan.discriminator.Discriminator.get_dis_matrices_rep -f qgan.generator.Generator.forward -f run_single_training run_single_training(CFG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
